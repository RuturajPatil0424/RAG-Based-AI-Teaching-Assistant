# ğŸ§  VectorMind â€“ RAG Based AI Assistant

VectorMind is a **local-first, Retrieval-Augmented Generation (RAG) based AI Assistant** designed to answer user queries accurately by combining **Large Language Models (LLMs)** with a **vector database** built from custom knowledge sources such as documents, websites, and videos.

Unlike traditional chatbots that rely only on pretrained knowledge, VectorMind retrieves **contextually relevant information** from your own data before generating responsesâ€”making answers **more accurate, explainable, and domain-specific**.

---

## ğŸš€ Key Features

- ğŸ” Retrieval-Augmented Generation (RAG)
- ğŸ§  Local LLM support (Ollama)
- ğŸ“š Vector database using embeddings
- âš¡ Streaming responses (token-by-token)
- ğŸ–¥ï¸ Custom GUI (desktop-based)
- ğŸ§© Modular & scalable architecture
- ğŸ”’ Offline & privacy-friendly
- ğŸ¥ Video-to-text knowledge ingestion
- ğŸ“„ Document chunking & semantic search

---

## â“ Problem Statement

### Traditional AI Assistants
- Hallucinate answers
- Cannot use custom or private data
- Depend heavily on cloud APIs
- Provide generic or outdated responses

### VectorMind Solves
- âŒ Hallucinations  
- âŒ Lack of domain-specific knowledge  
- âŒ Cloud dependency  
- âŒ Poor grounding of answers  

By **retrieving relevant content from a vector store before generation**, VectorMind ensures **fact-based, contextual, and reliable answers**.

---

## ğŸ§© System Architecture (High Level)

User Query<br>
â†“<br>
Query Embedding<br>
â†“<br>
Vector Store Search (FAISS)<br>
â†“<br>
Relevant Context Retrieval<br>
â†“<br>
Prompt Construction<br>
â†“<br>
Local LLM (Ollama)<br>
â†“<br>
Streaming Response to GUI


---
## ğŸ—‚ï¸ Project Structure

VectorMind/<br>
â”‚
â”œâ”€â”€ gui.py<br>
â”‚ â””â”€ Handles user interface (chat, streaming, theme, actions)<br>
â”‚
â”œâ”€â”€ chunking.py<br>
â”‚ â””â”€ Splits documents into semantic chunks<br>
â”‚
â”œâ”€â”€ vector_store.py<br>
â”‚ â””â”€ Creates & manages FAISS vector database<br>
â”‚
â”œâ”€â”€ local_llm.py<br>
â”‚ â””â”€ Connects to Ollama local models<br>
â”‚
â”œâ”€â”€ llm_answer.py<br>
â”‚ â””â”€ Standard response generation<br>
â”‚
â”œâ”€â”€ llm_answer_streaming.py<br>
â”‚ â””â”€ Token-by-token streaming responses<br>
â”‚
â”œâ”€â”€ video_to_mp3.py<br>
â”‚ â””â”€ Converts video files into audio/text<br>
â”‚
â”œâ”€â”€ requirements.txt<br>
â”‚ â””â”€ Project dependencies<br>
â”‚
â””â”€â”€ README.md<br>
---
âš™ï¸ Technologies Used

Python

Ollama (Local LLM runtime)

FAISS (Vector similarity search)

LangChain

Sentence Transformers / BGE / MiniLM

Tkinter / CustomTkinter (GUI)

Whisper / Speech-to-Text

NLTK

FFmpeg

ğŸ§ª Installation & Setup
### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/VectorMind.git
cd VectorMind

```

### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
python -m venv venv
venv\Scripts\activate   # Windows
source venv/bin/activate  # Linux/Mac

```
### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 24ï¸âƒ£ Install Ollama
## Download and install Ollama from:

```bash
https://ollama.com
```
## Pull a model:

```bash
ollama pull llama3:8b
# or
ollama pull llama3.2:3b
```

### 5ï¸âƒ£ Download Required NLTK Data

```bash
import nltk
nltk.download('punkt')
```

### â–¶ï¸ Running the Project
## Start Ollama Server

```bash
ollama serve
```

## Launch VectorMind
```bash
python gui.py
```
---
ğŸ’¬ How It Works (Step-by-Step)

- User enters a query

- Query is converted into an embedding

- FAISS retrieves top relevant chunks

- Context + query are merged into a prompt

- Local LLM generates an answer

- Tokens stream live into the GUI

- User can copy or regenerate responses

---

âœ… Advantages

- ğŸ”’ Privacy-first (no cloud calls)

- ğŸ“ˆ Highly accurate due to RAG

- âš¡ Fast local inference

- ğŸ”„ Reusable vector database

- ğŸ§  Custom knowledge support

- ğŸ› ï¸ Fully customizable

- ğŸ’» Works offline

---

âš ï¸ Current Limitations

- Requires local compute (RAM/GPU)

- Initial embedding generation takes time

- Large datasets increase indexing time

- GUI currently desktop-focused

---

ğŸ”® Future Enhancements

- ğŸŒ Web-based interface (Flask / FastAPI)

- ğŸ“ Multi-file batch upload

- ğŸ” Hybrid search (BM25 + Vector)

- ğŸ“Š Source citation & confidence score

- ğŸ§  Agent-based tool calling

- ğŸ—ƒï¸ SQLite / PostgreSQL metadata storage

- ğŸ” User authentication

- â˜ï¸ Optional cloud fallback

- ğŸ™ï¸ Voice-based interaction

- ğŸ§© Plugin system for tools

---

ğŸ¯ Ideal Use Cases

- AI Teaching Assistant

- Internal Knowledge Base

- Resume / Interview Prep Bot

- Research Assistant

- Company Documentation Chatbot

- Offline AI Assistant

- RAG Learning & Experimentation

---

ğŸ‘¤ Author

- Ruturaj Patil
- B.Tech Computer Engineering
- AI â€¢ ML â€¢ RAG â€¢ Python â€¢ Systems

---

â­ Final Note

- VectorMind is built as a production-ready learning project that demonstrates real-world RAG implementation, local LLM usage, and scalable AI system design.

- If youâ€™re learning RAG, LLM systems, or building private AI assistants, VectorMind is a strong foundation.